{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPD5grbdQoRhnHPIQm4amR4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Mathematics of Machine Learning\n","## Question Sheet 5 - Problem 5\n","\n","###Logistic Regression\n","Let $Y$ be a random variable that satisfies P(Y =1)=p, P(Y =0)=1−p.\n","This random variable models an event with two possible outcomes. The logistic model for p has the form\n","e⟨w, x⟩+b\n","p= 1+e⟨w,x⟩+b,\n","\n","x ∈ Rn is a vector of explanatory variables, and w ∈ Rn,b ∈ R are the model parameters that explain how p depends on the variables. For example, the variable Y could represent a choice in an election and the vector x encodes demographic information, or the variable Y could indicate the presence of a disease and x encodes data about a patient. The output is then taken to be 1 if p > 1/2 and 0 if p ⩽ 1/2.\n","Given vectors x1,...,xn and corresponding observed outcomes y1,...,yn, we would like to estimate the parameters w and b. Assuming the observed outcomes y1 = ··· = yk = 1 and yk+1 = ···yn = 0, the log likelihood function is defined as\n","kn\n","􏰍ln(pi)+ 􏰍 ln(1−pi), (2) i=1 i=k+1\n","where pi is the probability (1) computed using xi, and the goal is to find pa- rameters w, b that maximize this function. If we were allowed to choose the pi freely, then of course setting pi = 1 for 1 ⩽ i ⩽ k and pi = 0 else obviously maximizes this sum!\n","(a) Show that the negative of the log likelihood function is given by\n","kn\n","􏰍\n","f(w, b) = −\n","i=1\n","(⟨w, xi⟩ + b) +\n","􏰍 􏰃 ⟨w,xi⟩+b􏰄 ln 1 + e\n","i=1\n","Show that the sum of convex functions and the composition of a convex function with a linear one are convex, and deduce that the log likelihood function is convex.\n","(b) Consider the following table, in which the first row represents the hours of prepration and the second row whether an exam was passed (1) or\n","3\n","MA3K1 Mathematics of Machine Learning January 25, 2024\n","Hours 0.5 1 1.5 2 2.5 3 3 4.5 4 4.5 4.75 5 Pass 0001010111 11\n","not (0). Find the (negative) log likelihood function f (w, b). Solve the corresponding minimization problem\n","minf(w,b)\n","in two dimensions using either gradient descent (or any other method, such as Newton’s method, that you are aware of) and plot the result- ing probability function (1), giving the relationship of hours of study to probability of success.\n","  "],"metadata":{"id":"9hvtmp8m1_MB"}},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gULxATwF16mi","executionInfo":{"status":"ok","timestamp":1716994884568,"user_tz":-60,"elapsed":456,"user":{"displayName":"Oliver Brown","userId":"11927549576677789345"}},"outputId":"83936128-13cf-4bf8-b91d-405b2fae35d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 12)\n","27.75\n"]}],"source":["import numpy as np\n","data = np.array([[0.5,1,1.5,2,2.5,3,3,4.5,4,4.5,4.75,5],[0,0,0,1,0,1,0,1,1,1,1,1]])\n","print(data.shape)\n","\n","sum = 0\n","for k in range(12):\n","  if data[1][k] == 1:\n","    sum+=data[0][k]\n","\n","print(sum)\n","\n","def f(w,b):\n","  logsum = 0\n","  for k in range(12):\n","    logsum += np.log(1+np.exp(w*data[0][k]+b))\n","  return -(sum*(w+b)) + logsum\n","\n"]},{"cell_type":"markdown","source":["Makes sense, see her solution for details. Just implementing which isn't worth my time a week be"],"metadata":{"id":"gZQhWQEh5YuT"}}]}